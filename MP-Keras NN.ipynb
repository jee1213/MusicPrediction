{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the task is to categorize user given info (bd, city, registered_via, registration duration) \n",
    "# and songs (via language, genre_ids, )... such that the later given user can be categorized and \n",
    "# we can predict whether the user will like a given song or not.\n",
    "# One can choose to output predict_proba() for individual classifier and read out probability to be\n",
    "# classifies as 1 (=replay)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "songs = pd.read_csv('songs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user = pd.read_csv('members.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's fill up \"bd\" from user data alone. #\n",
    "user_bd_fill = user.drop('msno',axis=1)\n",
    "user_bd_fill = user_bd_fill.drop('registered_via',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_id = user['msno']\n",
    "user_reg = user['registered_via']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_train['bd'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#user_num.loc[(user_num['gender'].isnull()),'gender']=0\n",
    "#user_num['gender'] = user_num['gender'].map({0:0,'female':1,'male':2}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr,pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_bd_fill.loc[(user_bd_fill['bd']<0)|(user_bd_fill['bd']>100),'bd']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's convert this into day of year and year\n",
    "user_bd_fill['expiration_date'] = pd.to_datetime(user_bd_fill['expiration_date'],format='%Y%m%d')\n",
    "user_bd_fill['exp_year'] = user_bd_fill['expiration_date'].dt.year.astype(int)\n",
    "user_bd_fill['exp_doy'] = user_bd_fill['expiration_date'].dt.dayofyear.astype(int)\n",
    "user_bd_fill= user_bd_fill.drop('expiration_date',axis=1)\n",
    "user_bd_fill['registration_init_time'] = pd.to_datetime(user_bd_fill['registration_init_time'],format='%Y%m%d')\n",
    "user_bd_fill['reg_year'] = user_bd_fill['registration_init_time'].dt.year.astype(int)\n",
    "user_bd_fill['reg_doy'] = user_bd_fill['registration_init_time'].dt.dayofyear.astype(int)\n",
    "user_bd_fill = user_bd_fill.drop('registration_init_time',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.060697900431217984, pvalue=1.8826363771224628e-29)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearmanr(user_bd_fill['bd'],user_bd_fill['exp_doy'])\n",
    "#pearsonr(user_num['registration_init_time'],user_num['expiration_date'])\n",
    "#city:0.7847 / 0.5461\n",
    "#gender:0.8713 /0.691\n",
    "#registered_via:0.2274 /0.2266\n",
    "#reg_init_time:-0.5262 / -0.452\n",
    "#expiration_date:0.3371 / 0.119\n",
    "# pearsonr and spearmanr has similar correlation values.\n",
    "# let's drop gender\n",
    "# let's predict bd based on other user data (#registration_init_time, registered_via and expiration_date)\n",
    "# we use decision tree algorithm to fill in the missing bd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_bd_fill = user_bd_fill.drop('city',axis=1)\n",
    "user_bd_fill = user_bd_fill.drop('gender',axis=1)\n",
    "user_bd = user_bd_fill['bd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_bd_null = user_bd_fill.loc[user_bd_fill['bd']==0]\n",
    "user_bd_null = user_bd_null.drop('bd',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_bd_train = user_bd_fill.loc[user_bd_fill['bd']!=0]\n",
    "Xuser_bd_train = user_bd_train.drop('bd',axis=1)\n",
    "Yuser_bd_train = user_bd_train['bd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's normalize the user variables here.\n",
    "#user_bd_fill.head(5)\n",
    "scale = np.std(Xuser_bd_train)\n",
    "Xuser_bd_train /= scale\n",
    "user_bd_null /= scale \n",
    "mean = np.mean(Xuser_bd_train)\n",
    "Xuser_bd_train -= mean\n",
    "user_bd_null -= mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xuser_bd_train['registered_via'] = user.loc[user_bd_fill['bd']!=0,'registered_via']\n",
    "user_bd_null['registered_via'] = user.loc[user_bd_fill['bd']==0,'registered_via']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "exp_year          False\n",
       "exp_doy           False\n",
       "reg_year          False\n",
       "reg_doy           False\n",
       "registered_via    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xuser_bd_train.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.88\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(Xuser_bd_train, Yuser_bd_train)\n",
    "Y_pred = decision_tree.predict(user_bd_null)\n",
    "acc_decision_tree = round(decision_tree.score(Xuser_bd_train, Yuser_bd_train) * 100, 2)\n",
    "print(acc_decision_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_bd_null['bd'] = Y_pred\n",
    "Xuser_bd_train['bd']=Yuser_bd_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_bd_fill = pd.concat([user_bd_null,Xuser_bd_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34403"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_bd_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_bd_fill = user_bd_fill.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_bd_fill['msno'] = user['msno']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#song_info = pd.read_csv('song_extra_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we are not going to use additional song_info.\n",
    "#songs.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#let's drop unnecessary columns before merging, to save computational resources\n",
    "songs = songs.drop('lyricist',axis=1)\n",
    "songs = songs.drop('composer',axis=1)\n",
    "songs = songs.drop('artist_name',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scale = np.std(songs['song_length'])\n",
    "songs['song_length'] /= scale\n",
    "mean = np.mean(songs['song_length'])\n",
    "songs['song_length'] -= mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.merge(train,user_bd_fill,on='msno',how='left')\n",
    "df_test = pd.merge(test,user_bd_fill,on='msno',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#song_mg = pd.merge(songs,song_info,on='song_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = pd.merge(df_train,songs,on='song_id',how='left')\n",
    "df_test = pd.merge(df_test,songs,on='song_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2556790"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_train.loc[(df_train['bd']<0)|(df_train['bd']>100),'bd']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train does not miss an index. How did missing index happen in tidy_split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gender does not correlate much with othervariable : cannot be filled\n",
    "#user_gender_drop =  user_num[user_num.gender!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#spearmanr(user_gender_drop['gender'],user_gender_drop['reg_year'])\n",
    "#bd-registered_via 0.2258\n",
    "#bd-registration_init_time -0.332\n",
    "#bd-expiration_date 0.128\n",
    "# gender does not have significant correlation to any of the variables: we can drop gender? should we check it against song variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plt.hist(df_train['bd'])\n",
    "# too many unknown bds : about 40 % : instead of dropping them, find correlation and fill them in!\n",
    "# e.g. registered_via, registration_init_time, expiration_date-registration_init_time\n",
    "# is it different from including them in the training data separately?\n",
    "# it is, because it may induce false correlation if we simply decide all the unknown bds are 0s #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plt.hist(user_bd_fill['bd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_train.loc[(df_train['gender'].isnull()),'gender']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_train['gender'] = df_train['gender'].map({0:0,'female':1,'male':2}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test.loc[df_test['genre_ids'].isnull(),'genre_ids']=0\n",
    "df_train.loc[df_train['genre_ids'].isnull(),'genre_ids']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a fast way to split elements with given (esp unusual) delimeter\n",
    "def tidy_split(df, column, sep='|', keep=False):\n",
    "    \"\"\"\n",
    "    Split the values of a column and expand so the new DataFrame has one split\n",
    "    value per row. Filters rows where the column is missing.\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    df : pandas.DataFrame\n",
    "        dataframe with the column to split and expand\n",
    "    column : str\n",
    "        the column to split and expand\n",
    "    sep : str\n",
    "        the string used to split the column's values\n",
    "    keep : bool\n",
    "        whether to retain the presplit value as it's own row\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Returns a dataframe with the same columns as `df`.\n",
    "    \"\"\"\n",
    "    indexes = list()\n",
    "    new_values = list()\n",
    "    df = df.dropna(subset=[column])\n",
    "    for i, presplit in enumerate(df[column].astype(str)):\n",
    "        values = presplit.split(sep)\n",
    "        if keep and len(values) > 1:\n",
    "            indexes.append(i)\n",
    "            new_values.append(presplit)\n",
    "        for value in values:\n",
    "            indexes.append(i)\n",
    "            new_values.append(value)\n",
    "    new_df = df.iloc[indexes, :].copy()\n",
    "    new_df[column] = new_values\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_train[['genre1','genre2','genre3']] = df_train['genre_ids'].apply(lambda x: pd.Series(x.split('|')))\n",
    "#df_train[['genre_id']] = pd.DataFrame(df_train['genre_ids'].str.split('|').tolist()).head()\n",
    "df_train= tidy_split(df_train, 'genre_ids', sep='|')\n",
    "df_test= tidy_split(df_test, 'genre_ids', sep='|')\n",
    "# we will train the models, using all the duplicates\n",
    "# then we can average the target likelihood of duplicates to estimate the final target probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# genre_ids should not be normalized, as it is categorical and not continuous variable\n",
    "#df_train['genre_ids']=df_train['genre_ids'].astype(int)\n",
    "#df_test['genre_ids']=df_test['genre_ids'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#scale = np.std(df_train['genre_ids'])\n",
    "#df_train['genre_ids'] /= scale\n",
    "#df_test['genre_ids'] /= scale\n",
    "#mean = np.mean(df_train['genre_ids'])\n",
    "#df_train['genre_ids'] -= mean\n",
    "#df_test['genre_ids'] -= mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the duplicated index to dataframe\n",
    "test_idx = pd.DataFrame(df_test.index)\n",
    "train_idx = pd.DataFrame(df_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check what variable correlates with genre_ids\n",
    "#plt.scatter(df_train['bd'],df_train['registered_via'])\n",
    "#pd.crosstab(df_train['genre_cut'],df_train['gender'],normalize='columns')\n",
    "#spearmanr(df_train['genre_ids'],df_train['gender'])\n",
    "#somehow this correlation calculation (and crosstab) never finishes#\n",
    "#is it because it is too big?#\n",
    "# bd does play a role in preferred genre # song_length, language (a bit)\n",
    "# expiration_date does!!! WHY????\n",
    "# registered_via does a little\n",
    "# city does a littel too\n",
    "# registration_init_time not much\n",
    "# gender does not seem to #\n",
    "# are these gender - genre correlation at any statistical significance? how can I tell that? what is the standard deviation? #\n",
    "# for some genres, mixed gender fraction is significantly outside either one of the gender: does it mean it is small number stats#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# are song language and user city strongly correlated? - some exclusions do exist\n",
    "# is there a language strongly preferred in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_id = df_train['msno']\n",
    "test_id = df_test['msno']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2705361"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = df_train.drop('msno',axis=1)\n",
    "X_test = df_test.drop('msno',axis=1)\n",
    "X_train = X_train.drop('target',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X_train.drop('song_id',axis=1)\n",
    "X_test = X_test.drop('song_id',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.loc[X_train['language'].isnull(),'language']=0\n",
    "X_test.loc[X_test['language'].isnull(),'language']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_id = X_test['id']\n",
    "X_test = X_test.drop('id',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_train = pd.DataFrame(df_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.loc[X_train['source_system_tab'].isnull(),'source_system_tab']=0\n",
    "X_train.loc[X_train['source_screen_name'].isnull(),'source_screen_name']=0\n",
    "X_train.loc[X_train['source_type'].isnull(),'source_type']=0\n",
    "X_train.loc[X_train['song_length'].isnull(),'song_length']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test.loc[X_test['source_system_tab'].isnull(),'source_system_tab']=0\n",
    "X_test.loc[X_test['source_screen_name'].isnull(),'source_screen_name']=0\n",
    "X_test.loc[X_test['source_type'].isnull(),'source_type']=0\n",
    "X_test.loc[X_test['song_length'].isnull(),'song_length']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_train = len(X_train)\n",
    "n_test = len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X_test.loc[X_test['genre_ids'].isnull(),'genre_ids']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_tgt = pd.concat([X_train,X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tgt['src_sys_tab'] = pd.factorize(X_tgt['source_system_tab'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_tgt = X_tgt.drop('source_system_tab',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tgt['src_scr_nm'] = pd.factorize(X_tgt['source_screen_name'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_tgt = X_tgt.drop('source_screen_name',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tgt['src_typ'] = pd.factorize(X_tgt['source_type'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_tgt = X_tgt.drop('source_type',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp_year</th>\n",
       "      <th>exp_doy</th>\n",
       "      <th>reg_year</th>\n",
       "      <th>reg_doy</th>\n",
       "      <th>registered_via</th>\n",
       "      <th>bd</th>\n",
       "      <th>song_length</th>\n",
       "      <th>genre_ids</th>\n",
       "      <th>language</th>\n",
       "      <th>src_sys_tab</th>\n",
       "      <th>src_scr_nm</th>\n",
       "      <th>src_typ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.067646</td>\n",
       "      <td>0.577033</td>\n",
       "      <td>-0.068420</td>\n",
       "      <td>-1.651606</td>\n",
       "      <td>7</td>\n",
       "      <td>36</td>\n",
       "      <td>-0.251818</td>\n",
       "      <td>359</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.067646</td>\n",
       "      <td>0.298691</td>\n",
       "      <td>-0.370239</td>\n",
       "      <td>-0.383273</td>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "      <td>0.233597</td>\n",
       "      <td>1259</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.067646</td>\n",
       "      <td>0.298691</td>\n",
       "      <td>-0.370239</td>\n",
       "      <td>-0.383273</td>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "      <td>-0.134213</td>\n",
       "      <td>1259</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.067646</td>\n",
       "      <td>0.298691</td>\n",
       "      <td>-0.370239</td>\n",
       "      <td>-0.383273</td>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "      <td>0.052936</td>\n",
       "      <td>1019</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.067646</td>\n",
       "      <td>0.577033</td>\n",
       "      <td>-0.068420</td>\n",
       "      <td>-1.651606</td>\n",
       "      <td>7</td>\n",
       "      <td>36</td>\n",
       "      <td>-0.367832</td>\n",
       "      <td>1011</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   exp_year   exp_doy  reg_year   reg_doy  registered_via  bd  song_length  \\\n",
       "0  0.067646  0.577033 -0.068420 -1.651606               7  36    -0.251818   \n",
       "1  0.067646  0.298691 -0.370239 -0.383273               9  24     0.233597   \n",
       "2  0.067646  0.298691 -0.370239 -0.383273               9  24    -0.134213   \n",
       "3  0.067646  0.298691 -0.370239 -0.383273               9  24     0.052936   \n",
       "4  0.067646  0.577033 -0.068420 -1.651606               7  36    -0.367832   \n",
       "\n",
       "  genre_ids  language  src_sys_tab  src_scr_nm  src_typ  \n",
       "0       359      52.0            0           0        0  \n",
       "1      1259      52.0            1           1        1  \n",
       "2      1259      52.0            1           1        1  \n",
       "3      1019      -1.0            1           1        1  \n",
       "4      1011      52.0            0           0        0  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tgt.head(5)\n",
    "# categorical variables : registered_via, genre_ids, language, scr_sys_tab, src_scr_nm, src_typ\n",
    "# we should properly deal with them! : let's use binary encoder to turn them into categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder = ce.binary.BinaryEncoder(cols=['registered_via','genre_ids','language','src_sys_tab','src_scr_nm','src_typ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_tgt = encoder.fit_transform(X_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>registered_via_0</th>\n",
       "      <th>registered_via_1</th>\n",
       "      <th>registered_via_2</th>\n",
       "      <th>genre_ids_0</th>\n",
       "      <th>genre_ids_1</th>\n",
       "      <th>genre_ids_2</th>\n",
       "      <th>genre_ids_3</th>\n",
       "      <th>genre_ids_4</th>\n",
       "      <th>genre_ids_5</th>\n",
       "      <th>genre_ids_6</th>\n",
       "      <th>...</th>\n",
       "      <th>src_typ_0</th>\n",
       "      <th>src_typ_1</th>\n",
       "      <th>src_typ_2</th>\n",
       "      <th>src_typ_3</th>\n",
       "      <th>exp_year</th>\n",
       "      <th>exp_doy</th>\n",
       "      <th>reg_year</th>\n",
       "      <th>reg_doy</th>\n",
       "      <th>bd</th>\n",
       "      <th>song_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2556786</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.067646</td>\n",
       "      <td>0.310289</td>\n",
       "      <td>0.535219</td>\n",
       "      <td>-0.551793</td>\n",
       "      <td>24</td>\n",
       "      <td>-0.310257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556787</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.067646</td>\n",
       "      <td>0.310289</td>\n",
       "      <td>0.535219</td>\n",
       "      <td>-0.551793</td>\n",
       "      <td>24</td>\n",
       "      <td>-0.211555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556788</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.067646</td>\n",
       "      <td>0.310289</td>\n",
       "      <td>0.535219</td>\n",
       "      <td>-0.551793</td>\n",
       "      <td>24</td>\n",
       "      <td>-0.513171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556789</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.067646</td>\n",
       "      <td>0.310289</td>\n",
       "      <td>0.535219</td>\n",
       "      <td>-0.551793</td>\n",
       "      <td>24</td>\n",
       "      <td>-0.095958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         registered_via_0  registered_via_1  registered_via_2  genre_ids_0  \\\n",
       "2556786                 0                 0                 1            0   \n",
       "2556787                 0                 0                 1            0   \n",
       "2556788                 0                 0                 1            0   \n",
       "2556789                 0                 0                 1            0   \n",
       "\n",
       "         genre_ids_1  genre_ids_2  genre_ids_3  genre_ids_4  genre_ids_5  \\\n",
       "2556786            0            0            0            0            1   \n",
       "2556787            0            0            0            0            1   \n",
       "2556788            0            0            0            0            1   \n",
       "2556789            0            0            0            0            1   \n",
       "\n",
       "         genre_ids_6     ...       src_typ_0  src_typ_1  src_typ_2  src_typ_3  \\\n",
       "2556786            1     ...               0          0          0          0   \n",
       "2556787            1     ...               0          0          0          0   \n",
       "2556788            0     ...               0          0          0          0   \n",
       "2556789            1     ...               0          0          0          0   \n",
       "\n",
       "         exp_year   exp_doy  reg_year   reg_doy  bd  song_length  \n",
       "2556786  0.067646  0.310289  0.535219 -0.551793  24    -0.310257  \n",
       "2556787  0.067646  0.310289  0.535219 -0.551793  24    -0.211555  \n",
       "2556788  0.067646  0.310289  0.535219 -0.551793  24    -0.513171  \n",
       "2556789  0.067646  0.310289  0.535219 -0.551793  24    -0.095958  \n",
       "\n",
       "[4 rows x 34 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tgt.tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_tgt[:n_train]\n",
    "X_test = X_tgt[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.isnull().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isnull().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2705361"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(40,input_dim=34,activation='relu'))\n",
    "model.add(Dense(30,activation='relu'))\n",
    "model.add(Dense(20,activation='relu'))\n",
    "model.add(Dense(10,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train)\n",
    "Y_train = np.asarray(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "23s - loss: 0.6839 - acc: 0.5892\n",
      "Epoch 2/100\n",
      "22s - loss: 0.6531 - acc: 0.6219\n",
      "Epoch 3/100\n",
      "25s - loss: 0.6515 - acc: 0.6235\n",
      "Epoch 4/100\n",
      "24s - loss: 0.6505 - acc: 0.6243\n",
      "Epoch 5/100\n",
      "24s - loss: 0.6497 - acc: 0.6255\n",
      "Epoch 6/100\n",
      "25s - loss: 0.6489 - acc: 0.6268\n",
      "Epoch 7/100\n",
      "25s - loss: 0.6482 - acc: 0.6277\n",
      "Epoch 8/100\n",
      "24s - loss: 0.6476 - acc: 0.6284\n",
      "Epoch 9/100\n",
      "22s - loss: 0.6472 - acc: 0.6290\n",
      "Epoch 10/100\n",
      "24s - loss: 0.6469 - acc: 0.6293\n",
      "Epoch 11/100\n",
      "26s - loss: 0.6466 - acc: 0.6297\n",
      "Epoch 12/100\n",
      "23s - loss: 0.6465 - acc: 0.6298\n",
      "Epoch 13/100\n",
      "23s - loss: 0.6462 - acc: 0.6302\n",
      "Epoch 14/100\n",
      "26s - loss: 0.6461 - acc: 0.6304\n",
      "Epoch 15/100\n",
      "26s - loss: 0.6459 - acc: 0.6305\n",
      "Epoch 16/100\n",
      "24s - loss: 0.6457 - acc: 0.6306\n",
      "Epoch 17/100\n",
      "25s - loss: 0.6457 - acc: 0.6308\n",
      "Epoch 18/100\n",
      "26s - loss: 0.6454 - acc: 0.6310\n",
      "Epoch 19/100\n",
      "24s - loss: 0.6453 - acc: 0.6311\n",
      "Epoch 20/100\n",
      "25s - loss: 0.6452 - acc: 0.6312\n",
      "Epoch 21/100\n",
      "24s - loss: 0.6451 - acc: 0.6312\n",
      "Epoch 22/100\n",
      "27s - loss: 0.6450 - acc: 0.6314\n",
      "Epoch 23/100\n",
      "27s - loss: 0.6449 - acc: 0.6314\n",
      "Epoch 24/100\n",
      "27s - loss: 0.6448 - acc: 0.6315\n",
      "Epoch 25/100\n",
      "23s - loss: 0.6448 - acc: 0.6316\n",
      "Epoch 26/100\n",
      "26s - loss: 0.6448 - acc: 0.6315\n",
      "Epoch 27/100\n",
      "27s - loss: 0.6446 - acc: 0.6316\n",
      "Epoch 28/100\n",
      "26s - loss: 0.6445 - acc: 0.6318\n",
      "Epoch 29/100\n",
      "27s - loss: 0.6445 - acc: 0.6318\n",
      "Epoch 30/100\n",
      "27s - loss: 0.6445 - acc: 0.6319\n",
      "Epoch 31/100\n",
      "24s - loss: 0.6444 - acc: 0.6320\n",
      "Epoch 32/100\n",
      "24s - loss: 0.6443 - acc: 0.6321\n",
      "Epoch 33/100\n",
      "22s - loss: 0.6442 - acc: 0.6321\n",
      "Epoch 34/100\n",
      "22s - loss: 0.6442 - acc: 0.6321\n",
      "Epoch 35/100\n",
      "22s - loss: 0.6442 - acc: 0.6322\n",
      "Epoch 36/100\n",
      "22s - loss: 0.6441 - acc: 0.6323\n",
      "Epoch 37/100\n",
      "22s - loss: 0.6440 - acc: 0.6323\n",
      "Epoch 38/100\n",
      "22s - loss: 0.6440 - acc: 0.6324\n",
      "Epoch 39/100\n",
      "22s - loss: 0.6440 - acc: 0.6324\n",
      "Epoch 40/100\n",
      "22s - loss: 0.6439 - acc: 0.6325\n",
      "Epoch 41/100\n",
      "22s - loss: 0.6439 - acc: 0.6325\n",
      "Epoch 42/100\n",
      "22s - loss: 0.6438 - acc: 0.6325\n",
      "Epoch 43/100\n",
      "22s - loss: 0.6438 - acc: 0.6325\n",
      "Epoch 44/100\n",
      "22s - loss: 0.6437 - acc: 0.6326\n",
      "Epoch 45/100\n",
      "22s - loss: 0.6436 - acc: 0.6327\n",
      "Epoch 46/100\n",
      "22s - loss: 0.6436 - acc: 0.6327\n",
      "Epoch 47/100\n",
      "22s - loss: 0.6435 - acc: 0.6327\n",
      "Epoch 48/100\n",
      "22s - loss: 0.6435 - acc: 0.6328\n",
      "Epoch 49/100\n",
      "22s - loss: 0.6435 - acc: 0.6328\n",
      "Epoch 50/100\n",
      "22s - loss: 0.6434 - acc: 0.6329\n",
      "Epoch 51/100\n",
      "22s - loss: 0.6434 - acc: 0.6329\n",
      "Epoch 52/100\n",
      "22s - loss: 0.6433 - acc: 0.6330\n",
      "Epoch 53/100\n",
      "22s - loss: 0.6433 - acc: 0.6330\n",
      "Epoch 54/100\n",
      "22s - loss: 0.6433 - acc: 0.6330\n",
      "Epoch 55/100\n",
      "22s - loss: 0.6432 - acc: 0.6331\n",
      "Epoch 56/100\n",
      "22s - loss: 0.6432 - acc: 0.6331\n",
      "Epoch 57/100\n",
      "22s - loss: 0.6432 - acc: 0.6332\n",
      "Epoch 58/100\n",
      "22s - loss: 0.6431 - acc: 0.6332\n",
      "Epoch 59/100\n",
      "22s - loss: 0.6431 - acc: 0.6332\n",
      "Epoch 60/100\n",
      "22s - loss: 0.6431 - acc: 0.6332\n",
      "Epoch 61/100\n",
      "22s - loss: 0.6430 - acc: 0.6333\n",
      "Epoch 62/100\n",
      "22s - loss: 0.6429 - acc: 0.6334\n",
      "Epoch 63/100\n",
      "22s - loss: 0.6429 - acc: 0.6333\n",
      "Epoch 64/100\n",
      "22s - loss: 0.6430 - acc: 0.6334\n",
      "Epoch 65/100\n",
      "22s - loss: 0.6429 - acc: 0.6334\n",
      "Epoch 66/100\n",
      "22s - loss: 0.6428 - acc: 0.6334\n",
      "Epoch 67/100\n",
      "22s - loss: 0.6428 - acc: 0.6335\n",
      "Epoch 68/100\n",
      "22s - loss: 0.6428 - acc: 0.6335\n",
      "Epoch 69/100\n",
      "22s - loss: 0.6427 - acc: 0.6336\n",
      "Epoch 70/100\n",
      "22s - loss: 0.6428 - acc: 0.6336\n",
      "Epoch 71/100\n",
      "22s - loss: 0.6427 - acc: 0.6335\n",
      "Epoch 72/100\n",
      "22s - loss: 0.6426 - acc: 0.6337\n",
      "Epoch 73/100\n",
      "22s - loss: 0.6426 - acc: 0.6337\n",
      "Epoch 74/100\n",
      "22s - loss: 0.6427 - acc: 0.6336\n",
      "Epoch 75/100\n",
      "22s - loss: 0.6425 - acc: 0.6337\n",
      "Epoch 76/100\n",
      "22s - loss: 0.6426 - acc: 0.6337\n",
      "Epoch 77/100\n",
      "22s - loss: 0.6426 - acc: 0.6336\n",
      "Epoch 78/100\n",
      "22s - loss: 0.6425 - acc: 0.6338\n",
      "Epoch 79/100\n",
      "22s - loss: 0.6424 - acc: 0.6338\n",
      "Epoch 80/100\n",
      "22s - loss: 0.6425 - acc: 0.6337\n",
      "Epoch 81/100\n",
      "22s - loss: 0.6425 - acc: 0.6338\n",
      "Epoch 82/100\n",
      "22s - loss: 0.6424 - acc: 0.6338\n",
      "Epoch 83/100\n",
      "22s - loss: 0.6424 - acc: 0.6337\n",
      "Epoch 84/100\n",
      "22s - loss: 0.6424 - acc: 0.6339\n",
      "Epoch 85/100\n",
      "22s - loss: 0.6423 - acc: 0.6339\n",
      "Epoch 86/100\n",
      "22s - loss: 0.6423 - acc: 0.6339\n",
      "Epoch 87/100\n",
      "22s - loss: 0.6424 - acc: 0.6338\n",
      "Epoch 88/100\n",
      "22s - loss: 0.6422 - acc: 0.6339\n",
      "Epoch 89/100\n",
      "22s - loss: 0.6423 - acc: 0.6339\n",
      "Epoch 90/100\n",
      "22s - loss: 0.6422 - acc: 0.6340\n",
      "Epoch 91/100\n",
      "22s - loss: 0.6422 - acc: 0.6340\n",
      "Epoch 92/100\n",
      "22s - loss: 0.6422 - acc: 0.6340\n",
      "Epoch 93/100\n",
      "22s - loss: 0.6422 - acc: 0.6339\n",
      "Epoch 94/100\n",
      "22s - loss: 0.6421 - acc: 0.6341\n",
      "Epoch 95/100\n",
      "22s - loss: 0.6421 - acc: 0.6340\n",
      "Epoch 96/100\n",
      "22s - loss: 0.6421 - acc: 0.6340\n",
      "Epoch 97/100\n",
      "22s - loss: 0.6421 - acc: 0.6341\n",
      "Epoch 98/100\n",
      "22s - loss: 0.6420 - acc: 0.6341\n",
      "Epoch 99/100\n",
      "22s - loss: 0.6421 - acc: 0.6340\n",
      "Epoch 100/100\n",
      "22s - loss: 0.6420 - acc: 0.6341\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=100, batch_size=50000,  verbose=2)\n",
    "# batch size ~ 1% of the training dataset\n",
    "# 10 iterations\n",
    "# calculate predictions\n",
    "X_test = np.asarray(X_test)\n",
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X_test.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ijee/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.39\n"
     ]
    }
   ],
   "source": [
    "#random_forest = RandomForestClassifier(n_estimators=20, max_depth=30)\n",
    "#random_forest.fit(X_train, Y_train)\n",
    "#Y_pred = clf.predict(X_test)\n",
    "#print clf.score(X_train, Y_train)\n",
    "#acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\n",
    "#print (acc_random_forest)\n",
    "# accuracy too high : indicates that it has high variance (overfitting) problem. \n",
    "# Remember even the best models in the competition performs about 70 % success on the test dataset.\n",
    "# best performance so far : RF w/ depth 30 est 20, normalized (minor improvement over depth 20 / not normalized case)\n",
    "# now let's implement NN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Y_pred = random_forest.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_pred = pd.DataFrame(Y_pred)\n",
    "Y_pred['index'] =test_idx\n",
    "Y_pred = Y_pred.groupby('index').mean()\n",
    "#Y_pred = Y_pred[~Y_pred.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_pred.columns=['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# modify this such that it won't discard, but average, the duplicates\n",
    "#df_train = df_train[~df_train.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modify this such that it won't discard, but average, the duplicates\n",
    "#df_test = df_test[~df_test.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "            \"id\": test_id,\n",
    "                    \"target\": Y_pred['pred']\n",
    "    })\n",
    "submission = submission.groupby('id').mean().reset_index()\n",
    "\n",
    "submission.to_csv('NNsubmission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(test) !=len(submission):\n",
    "    print('something is wrong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
